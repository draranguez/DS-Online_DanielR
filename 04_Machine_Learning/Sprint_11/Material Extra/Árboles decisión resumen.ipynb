{"cells":[{"cell_type":"markdown","metadata":{"id":"O_3YMwzQ5SB8"},"source":["![image.png](attachment:image.png)"]},{"cell_type":"markdown","metadata":{"id":"zytiB_Rc5SCA"},"source":["# **Arboles de Decisión**"]},{"cell_type":"markdown","metadata":{"id":"-A-NCsuS5SCA"},"source":["\n","\n","Los **Árboles de Decisión** son un tipo de modelo predictivo utilizado en estadísticas, minería de datos y aprendizaje automático. Son modelos versátiles que se pueden usar tanto para tareas de clasificación como de regresión. Un árbol de decisión representa una serie de decisiones basadas en las características de los datos que pueden llevar a una predicción o clasificación final.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"duqqfsj15SCB"},"source":["## Invocando el Modelo de Árboles de Decisión en Scikit-Learn\n","\n","Para utilizar los modelos de Árboles de Decisión en Scikit-Learn, primero debes importar las clases `DecisionTreeClassifier` o `DecisionTreeRegressor` desde el módulo `tree`. Luego, puedes instanciar el modelo, entrenarlo con los datos y utilizarlo para hacer predicciones.\n","\n","Aquí tienes un ejemplo de cómo hacerlo para clasificación:\n","\n","```python\n","from sklearn.tree import DecisionTreeClassifier # Y para regresión, el proceso es similar pero utilizando 'DecisionTreeRegressor'\n","from sklearn.model_selection import train_test_split\n","\n","# Datos de ejemplo\n","X, y = datos_independientes, datos_dependientes\n","\n","# Dividir los datos en conjuntos de entrenamiento y prueba\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Instanciar y entrenar el modelo\n","modelo = DecisionTreeClassifier()\n","modelo.fit(X_train, y_train)\n","\n","# Hacer predicciones\n","y_pred = modelo.predict(X_test)\n","\n","# y_pred contiene las predicciones de clase para el conjunto de prueba\n"]},{"cell_type":"markdown","metadata":{"id":"bRwwSZNi5SCB"},"source":["## Hiperparámetros más destacados de los Árboles de Decisión en Scikit-Learn (Actualizada)\n","\n","Los modelos de Árboles de Decisión vienen con varios hiperparámetros que permiten afinar su comportamiento. Algunos de los más destacados incluyen:\n","\n","- `criterion`: La función para medir la calidad de una división. \"gini\" para la impureza de Gini y \"entropy\" para la ganancia de información.\n","\n","- `splitter`: La estrategia utilizada para elegir la división en cada nodo. Los valores admitidos son \"best\" para elegir la mejor división y \"random\" para elegir la mejor división aleatoria.\n","\n","- `max_depth`: La máxima profundidad del árbol. Si es None, los nodos se expanden hasta que todas las hojas son puras o hasta que todas las hojas contienen menos muestras que `min_samples_split`.\n","\n","- `min_samples_split`: El número mínimo de muestras requeridas para dividir un nodo interno.\n","\n","- `min_samples_leaf`: El número mínimo de muestras requeridas para estar en un nodo hoja.\n","\n","- `max_features`: El número de características a considerar al buscar la mejor división.\n","\n","- `class_weight`: Ponderaciones asociadas con clases. Si no se proporciona, todas las clases se suponen que tienen peso uno. En el caso de 'balanced', las ponderaciones se ajustan automáticamente inversamente proporcionales a las frecuencias de clase en los datos de entrada.\n","\n","Estos son solo algunos de los hiperparámetros disponibles. La elección de estos hiperparámetros puede afectar significativamente el rendimiento del modelo.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"oMunSB9k5SCC"},"source":["## Hiperparámetros comúnmente optimizados con GridSearchCV o RandomizedSearchCV\n","\n","Al optimizar un modelo de Árboles de Decisión, los siguientes hiperparámetros son comúnmente ajustados mediante técnicas como `GridSearchCV` o `RandomizedSearchCV`:\n","\n","Para encontrar el punto optimo entre overfitting y underfitting (recuerda que los árboles son muy propensos a lo primero si se complican y muy dados a lo primero si no se ajustan bien los hiperparámetros siguientes):\n","\n","- `max_depth`: Es útil probar diferentes profundidades para controlar la complejidad del modelo y prevenir sobreajuste.\n","\n","- `min_samples_split` y `min_samples_leaf`: Ajustar estos parámetros ayuda a prevenir el sobreajuste y asegura que cada hoja del árbol tenga suficientes muestras.\n","\n","Para mejorar el entrenamiento:\n","\n","- `max_features`: Limitar el número de características puede mejorar el rendimiento y reducir el tiempo de entrenamiento.\n","\n","En situaciones de desbalanceo de la clase target en problemas de clasificación:\n","\n","- `class_weight`: Es especialmente importante cuando se trabaja con conjuntos de datos que tienen clases desbalanceadas. Puede ser útil probar diferentes estrategias de ponderación de clases.\n","\n","En general, requieren siempre una optimización de los tres primeros.\n","\n","NOTA: Esta es una sugerencia y no una imposición, si tienes tiempo y lo crees conveniente puede ser bueno probar el resto de hiperparámetros que puedes encontrar en la documentación de sklearn, [para clasificación](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier) y [para regresión](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor)\n"]},{"cell_type":"markdown","metadata":{"id":"fWyUC66m5SCC"},"source":["## Sugerencias de Cuándo Aplicar los Árboles de Decisión\n","\n","Los Árboles de Decisión son modelos versátiles que pueden ser útiles en una variedad de situaciones. Son especialmente apropiados cuando:\n","\n","1. **Interpretabilidad es importante**: Son fáciles de entender e interpretar, y la visualización de árboles puede ser muy informativa.\n","\n","2. **Relaciones no lineales**: Pueden capturar relaciones no lineales entre las características y la variable objetivo.\n","\n","3. **No requiere escalamiento de características**: A diferencia de muchos otros modelos, los árboles de decisión no requieren que las características tengan una escala específica.\n","\n","4. **Datos con mezcla de características**: Pueden manejar datos con una mezcla de características numéricas y categóricas.\n","\n","Sin embargo, es importante tener en cuenta que los árboles de decisión pueden ser propensos al sobreajuste, especialmente si el árbol es muy profundo. Por lo tanto, es crucial ajustar los hiperparámetros adecuadamente y considerar técnicas como el ensamblado para mejorar el rendimiento.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"plaintext"},"id":"Y0GBgTbL5SCD"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"gaDwlbLK5SCE"},"source":[]},{"cell_type":"markdown","metadata":{"id":"mAtmN0vE5SCE"},"source":[]}],"metadata":{"language_info":{"name":"python"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}